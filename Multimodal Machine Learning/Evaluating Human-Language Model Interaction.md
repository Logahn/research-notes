# Evaluating Human-Language Model Interaction

## Tasks: 

1. Propose an alternative title.
2. Discuss a missing experiment from the paper.
3. Submit a question about the paper.
4. What social or other impacts were overlooked or omitted?
5. Role play (Peer reviewer) - The paper has not been published yet and is currently submitted to a top conference where youâ€™ve been assigned as a peer reviewer. Complete a full review of the paper answering all prompts of the official review form of the top venue in this research area. This includes recommending whether to accept or reject the paper. (2 students, 10min)



The current language models are primarily evaluated *non-interactively.*
Proposed title: **Human-AI Language-based Interaction Evaluation**



**Alternative title:**
Human-AI Language-based Interaction Evaluation (HALIE)

**Missing Experiments from the paper:**
- The paper stated that in scenerios where there is interaction between an LM and multiple users or between an LM and another LM, there may be a need to extend the framework.*(Evaluation of LM to LM or LM to multiple users)*
- A comparison of HALIE framework's effectiveness with other existing evaluation frameworks could help highlight HALIE's strength and weaknesses.

**Question About the Paper:**
How do the differences between HALIE and non-interactive methods affect the evaluation of human-LM interactions?

**Social and Other Impacts overlooked:** 
- Although the paper mentioned that generating toxic contents can cause psychological harm, the paper completely leaves out how cultural nuances and biases can affect the interaction with the user. 


### Peer review based on the Association for Computational Linguistics Rolling Review Guidelines

**Peer review points:**
- The experiments and evaluations, though extensive, were performed on specifically selected tasks. 

**Paper Summary**
The authors were able to evaluate human-LM interaction instead of just model completions.

**Summary of Strengths:**
- It challenges conventional, non-interactive evaluation metrics (best paper award)
- Provided guidelines to applying the framework to new scenrios and publicly released data.
- Framework is designed to be a reference for researchers when designing the evaluation of human-LM interaction in *new scenerios.*

**Summary of Weaknesses:**
Although the paper mentioned that generating toxic contents can cause psychological harm, the paper completely leaves out how cultural nuances and biases can affect the interaction with the user. 

**Soundness:**
4.5 = Quite Strong: This study provides sufficient support for all of its claims/arguments and it shows a high level understanding of the research area.

**Overall Assessment:**
4 = The paper represents solid work, and is of significant interest for the communities that might build on it.