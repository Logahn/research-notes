Machine Learning Fall 2024

Machine Learning is a computer learn from experience E
with respect to a specific task T
and a specific problem P
if its performance on T measured by P improves with E

Office hours - Tuesdays and Thursday 10:00 - 11:00
TA's place - 6th floor 25PP

Supervised learning makes a rule from experience and uses this rule to make predictions
Labeled data are the results of an experience. 

Unsupervised learning is without experience and label and the machine decides to classify based on the similarities between data.

Surface and Deep Learning

Training distribution should commensurate testing distribution
You cannot use the experience of world champions to train a model for elementary school kids


All machine algorithms can be boiled down to finding the optimal weights for the model


Step size

Concept learning

Concept learning is like trying to describe the world to the computer
Consider concept learning like simulating the learning process of a baby in a machine.



FInding the maximally general bound for concept learning

## Concept Learning:

## Find S Algorithm:

## Decision Tree Learning:

Decision trees is approximating discrete-values functions. You have to convert categorical values to discrete values for decision trees.
What is ID3?

Definition of Decision trees:
Decision trees classify instances by sorting them from root to leaf nodes.
Decision tree represents the disjunction of the conjuction of the constraints on attribute values of instances.

Given the training data, the model should be able to build a decision tree.
it uses a top-down greedy algorithm (trying to find the fastest not the best)

ID3 Algorithm is a topdown algorithm and we need to determine which attribute should be tested firsr and there is never backtracks and it is a greedy algorithm.

Determinbation of best attribute:
Information gain - How well an attribute separates the training examples.
Entropy - the purity of an arbitrary collection of exampkes. *-p1 log2*p1 - p0 log2*p0* Entropy tells us when the percentahe of each attribute to each other. If they are similar, entropy is large.

Best attribute partitions data set to groups with small entropy. Check information gain calculations.

Development of decision trees
- Find the information gain of eah attribute.

