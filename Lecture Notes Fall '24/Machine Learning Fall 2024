Machine Learning Fall 2024

Machine Learning is a computer learn from experience E
with respect to a specific task T
and a specific problem P
if its performance on T measured by P improves with E

Office hours - Tuesdays and Thursday 10:00 - 11:00
TA's place - 6th floor 25PP

Supervised learning makes a rule from experience and uses this rule to make predictions
Labeled data are the results of an experience.

Unsupervised learning is without experience and label and the machine decides to classify based on the similarities between data.

Surface and Deep Learning

Training distribution should commensurate testing distribution
You cannot use the experience of world champions to train a model for elementary school kids

All machine algorithms can be boiled down to finding the optimal weights for the model

Step size

Concept learning

Concept learning is like trying to describe the world to the computer
Consider concept learning like simulating the learning process of a baby in a machine.

FInding the maximally general bound for concept learning

## Concept Learning:

## Find S Algorithm:

## Decision Tree Learning:

Decision trees is approximating discrete-values functions. You have to convert categorical values to discrete values for decision trees.
What is ID3?

Definition of Decision trees:
Decision trees classify instances by sorting them from root to leaf nodes.
Decision tree represents the disjunction of the conjuction of the constraints on attribute values of instances.

Given the training data, the model should be able to build a decision tree.
it uses a top-down greedy algorithm (trying to find the fastest not the best)

ID3 Algorithm is a topdown algorithm and we need to determine which attribute should be tested firsr and there is never backtracks and it is a greedy algorithm.
ID3 means Iterative Dechotomiser

- Top-down approach: ID3 starts with the root node and recursively splits the data into smaller subsets based on the most informative attribute.
- Greedy algorithm: ID3 uses a greedy approach to select the best attribute to split the data at each node. It chooses the attribute that results in the largest information gain.
- No backtracking: ID3 does not perform backtracking, meaning that once a decision is made at a node, it is not revisited.

The ID3 algorithm works as follows:

Select the root node: Choose the attribute with the highest information gain as the root node.
Split the data: Split the data into two subsets based on the selected attribute.
Recursively split: Recursively apply the ID3 algorithm to each subset until a stopping criterion is met (e.g., all instances belong to the same class).
Create a leaf node: Create a leaf node for each subset and assign the corresponding class label.
ID3 is a simple and efficient algorithm for building decision trees, but it has some limitations, such as:

Overfitting: ID3 can result in overfitting, especially when dealing with noisy or high-dimensional data.
Handling missing values: ID3 does not handle missing values well, which can lead to biased results.
Despite these limitations, ID3 remains a popular algorithm for building decision trees and is often used as a baseline for more advanced algorithms.

Determination of best attribute:
Information gain - How well an attribute separates the training examples.
Entropy - the purity of an arbitrary collection of exampkes. *-p1 log2*p1 - p0 log2*p0* Entropy tells us when the percentage of each attribute to each other. If they are similar, entropy is large.

Best attribute partitions data set to groups with small entropy. Check information gain calculations.

Development of decision trees

- Find the information gain of eah attribute.

Entropy = -∑(p × log2(p)) (smaller is better)
Information gain = Entropy(parent) - ∑( (|child| / |parent|) × Entropy(child) ) (higher is better)
Gini index => Two randomly selected items must be if same class and the probability for this is 1, if the population is pure.
Gini index = 1 - ∑(p^2)
Gini impurity => 1 - (probability of positive)^2 - (probability of negative)^2

ID3 doesnt give acecess to control the size of the tree. Overfitting is when thr training accuracy is sohigh yet the testing accuracy is very low, because you have over adjusted the model to be accurate only on the training dataset.

If an input is not linearly separable, a perceptron is not useful in that case.

Research Title: Building Cyberspace Infrastructure for SPace Weather Analytics
Speaker: Dr. Berkay Ardyn
Description of Talk:

Problems -
Solar flare and Radiations, Geomagnetic storms
Space crew safety
Navigations error and commubnication error
Aviation safety

Cause: Dynamic and constant solar effect on earth. Numerous phenomena have intertwined triggers, all having different magnitudes from flares to solar storms.

Approach: Definition of solar active regions
Data (Acquisition, storage, processing)
MOdel (Training, ensembling, optimization, validation)
System (Developing, testing, integration, deployment)

Cyber infrastructures are
Data acquisition
Data storage
Data mitigation
Data mining and machine learning

Traditional methods are satellites, observation and raw data(server)
Novel methods are Data providers, analytics software and prediction products

Solar data sources are

- Satelites
- Raw data
- Rasters (images, magnetograms)
- Time series
- Metadata
- AR metadata
- Flares, CME, SEP

Multifaceted data
Highdimensional data
Heterogenous data

Cyberstructures are currently used to solve problems
Approach -> Creation of benchmark datasets
Comparison models
Creation of space weather analytics Datasets(SWAN-SF)
Creating mnultivariate timeseries for each active trajectory

Space weather forecasting - solar weather forecasting

Start from active regions -> Predict flares -> predict CME

FLare production algo: Multivariate or image classification Used together to complement

CME - Corona Mass Ejection
SEP - Solar Energetic Particles

Tuesday 17th Septmeber '24
Perceptrons are single unit ANNs
