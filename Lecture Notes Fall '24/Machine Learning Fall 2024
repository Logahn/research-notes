Machine Learning Fall 2024

Machine Learning is a computer learn from experience E
with respect to a specific task T
and a specific problem P
if its performance on T measured by P improves with E

Office hours - Tuesdays and Thursday 10:00 - 11:00
TA's place - 6th floor 25PP

Supervised learning makes a rule from experience and uses this rule to make predictions
Labeled data are the results of an experience. 

Unsupervised learning is without experience and label and the machine decides to classify based on the similarities between data.

Surface and Deep Learning

Training distribution should commensurate testing distribution
You cannot use the experience of world champions to train a model for elementary school kids


All machine algorithms can be boiled down to finding the optimal weights for the model


Step size

Concept learning

Concept learning is like trying to describe the world to the computer
Consider concept learning like simulating the learning process of a baby in a machine.



FInding the maximally general bound for concept learning

## Concept Learning:

## Find S Algorithm:

## Decision Tree Learning:

Decision trees is approximating discrete-values functions. You have to convert categorical values to discrete values for decision trees.
What is ID3?

Definition of Decision trees:
Decision trees classify instances by sorting them from root to leaf nodes.
Decision tree represents the disjunction of the conjuction of the constraints on attribute values of instances.

Given the training data, the model should be able to build a decision tree.
it uses a top-down greedy algorithm (trying to find the fastest not the best)

ID3 Algorithm is a topdown algorithm and we need to determine which attribute should be tested firsr and there is never backtracks and it is a greedy algorithm.
ID3 means Iterative Dechotomiser

- Top-down approach: ID3 starts with the root node and recursively splits the data into smaller subsets based on the most informative attribute.
- Greedy algorithm: ID3 uses a greedy approach to select the best attribute to split the data at each node. It chooses the attribute that results in the largest information gain.
- No backtracking: ID3 does not perform backtracking, meaning that once a decision is made at a node, it is not revisited.

The ID3 algorithm works as follows:

Select the root node: Choose the attribute with the highest information gain as the root node.
Split the data: Split the data into two subsets based on the selected attribute.
Recursively split: Recursively apply the ID3 algorithm to each subset until a stopping criterion is met (e.g., all instances belong to the same class).
Create a leaf node: Create a leaf node for each subset and assign the corresponding class label.
ID3 is a simple and efficient algorithm for building decision trees, but it has some limitations, such as:

Overfitting: ID3 can result in overfitting, especially when dealing with noisy or high-dimensional data.
Handling missing values: ID3 does not handle missing values well, which can lead to biased results.
Despite these limitations, ID3 remains a popular algorithm for building decision trees and is often used as a baseline for more advanced algorithms.

Determination of best attribute:
Information gain - How well an attribute separates the training examples.
Entropy - the purity of an arbitrary collection of exampkes. *-p1 log2*p1 - p0 log2*p0* Entropy tells us when the percentage of each attribute to each other. If they are similar, entropy is large.

Best attribute partitions data set to groups with small entropy. Check information gain calculations.

Development of decision trees
- Find the information gain of eah attribute.

Entropy = -∑(p × log2(p)) (smaller is better)
Information gain = Entropy(parent) - ∑( (|child| / |parent|) × Entropy(child) ) (higher is better)
Gini index => Two randomly selected items must be if same class and the probability for this is 1, if the population is pure.
Gini index  = 1 - ∑(p^2)
Gini impurity => 1 - (probability of positive)^2 - (probability of negative)^2
